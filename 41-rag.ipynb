{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e45b5c4e-5f4b-4a5b-b220-35ddf1eb6096",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35a467-ea23-4416-b0b8-77cfae832288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c19a972-0503-41c0-ab4b-e5246ac929a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model_name = 'Qwen/Qwen2.5-0.5B-Instruct'\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name, \n",
    "                                             attn_implementation='flash_attention_2',\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"cuda\",\n",
    "                                             low_cpu_mem_usage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e39f01-92c6-47f6-8905-e2f0f6f7e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8d5cb-b581-4a4e-bc3c-c2806bd3513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1977996-e538-4a60-a43a-bf509e3e303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890ded6d-5962-4617-9b8a-a22e183f3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{ \"content\": \"Du bist ein hilfreicher Assistent.\", \n",
    "              \"role\": \"system\" }, \n",
    "            { \"content\": \"Erkl√§re den Heise Zeitschriftenverlag!\", \n",
    "              \"role\": \"user\" },\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92561b1-ce14-47ee-a8a3-f9cbfde8727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = gen_tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors = \"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5226b1-fd05-4ad2-adec-90b012bb5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gen_tokenizer.batch_decode(inputs)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e295d30-5690-44a4-a992-4a7787d1975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = gen_model.generate(inputs, max_new_tokens = 512, use_cache = True,\n",
    "                         do_sample=True, temperature=0.7, top_k=25, top_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fc640-604a-4840-b1ac-eb69baabda6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(gen_tokenizer.batch_decode(outputs)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddad02-fea3-40a0-b9b2-eb4ad04f910b",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f0989-f0fc-4ece-be8e-3a66ab42b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ff6ff-ef13-43b3-babf-8d4d8121d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Snowflake/snowflake-arctic-embed-l-v2.0\"\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667c0e6-6d8b-446c-a01c-7ff2ac81921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"llm-abstract-sentences-saev2.npy\",  \"rb\") as f:\n",
    "    embeddings = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444c400-0c59-40ae-b1d1-30da92725dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "with lzma.open(\"llm-abstract-sentences.json.xz\", \"rt\") as f:\n",
    "    es = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f410314-e4aa-457d-815a-7fcd858f2487",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [e[\"title\"] + \": \" + e[\"text\"] for e in es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a382-cf9f-4e9d-a154-a811479d22ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross encoder\n",
    "from sentence_transformers import CrossEncoder, util\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416ac725-97c8-4709-aad3-f36cac16a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def search(query, text, embeddings, bi_encoder, cross_encoder, top=100):\n",
    "    question_embedding = bi_encoder.encode(query, normalize_embeddings=True, prompt_name=\"query\")\n",
    "    \n",
    "    sim = model.similarity(question_embedding, embeddings).flatten().numpy() \n",
    "    \n",
    "    hits = [ { \"text\": text[i], \"score\": sim[i] } \n",
    "                     for i in sim.argsort()[::-1][0:top] ]\n",
    "\n",
    "    # Consider only top hits for re-ranking\n",
    "    cross_input = [[query, hit[\"text\"]] for hit in hits]\n",
    "    # cross-encode (this takes most time)\n",
    "    cross_scores = cross_encoder.predict(cross_input)\n",
    "\n",
    "    # Integrate cross-scores in original hits (this would be easier with pandas)\n",
    "    for i in range(len(cross_scores)):\n",
    "        hits[i][\"cross-score\"] = cross_scores[i]\n",
    "\n",
    "    # re-sort by cross-score, descending!\n",
    "    hits = sorted(hits, key=lambda x: x[\"cross-score\"], reverse=True)\n",
    "    \n",
    "    # Return top-20 results of re-ranker as dataframe\n",
    "    return pd.DataFrame(hits[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41ac5f-f641-49c4-8090-fe99a9d0fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, text, embeddings, bi_encoder, cross_encoder, gen_model, gen_tokenizer, top=100, top_rag=5):\n",
    "    res = search(query, text, embeddings, bi_encoder, cross_encoder, top)\n",
    "    context = \"\\n\".join(res[\"text\"].map(str).values[0:top_rag])\n",
    "    messages = [{ \"content\": \"\"\"You are an assistant which answers questions only based on the context.\n",
    "                            If the answer is not in the context, say that you can't answer the question.\n",
    "                            Use correct scientific terms.\"\"\", \n",
    "              \"role\": \"system\" }, \n",
    "            { \"content\": f\"Answer the question '{query}' based on the context: {context}\", \n",
    "              \"role\": \"user\" },\n",
    "            ]\n",
    "    inputs = gen_tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = gen_model.generate(inputs, max_new_tokens=512, use_cache=True, do_sample=False)\n",
    "    gen = gen_tokenizer.batch_decode(outputs)[0]\n",
    "    return gen[gen.find(\"<|im_start|>assistant\"):], res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bacdfc-419a-4226-aa01-b77969e10e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, df = rag(\"How long do I train an LLM?\", sentences, embeddings, model, cross_encoder, gen_model, gen_tokenizer)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6dfb92-63fb-4110-a5b8-20ad8ecd5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, df = rag(\"How long was Llama 3.2 trained?\", sentences, embeddings, model, cross_encoder, gen_model, gen_tokenizer)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e8cfc-10ba-42ce-99e1-d28605c69860",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, df = rag(\"How does SGD work?\", sentences, embeddings, model, cross_encoder, gen_model, gen_tokenizer)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4db697-dd50-481e-bcde-85b772c380a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
