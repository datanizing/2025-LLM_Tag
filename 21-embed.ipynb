{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba15b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbbe2e0-204d-441b-962b-d66e9f0f24e3",
   "metadata": {},
   "source": [
    "Modell ermitteln: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6da7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"jinaai/jina-embeddings-v3\"                 # am schnellsten\n",
    "#model_name = \"intfloat/multilingual-e5-large-instruct\"   # etwas langsamer\n",
    "model_name = \"Snowflake/snowflake-arctic-embed-l-v2.0\"    # ca. genau so schnell\n",
    "#model_name = \"infly/inf-retriever-v1-1.5b\"               # etwa 3x langsamer\n",
    "\n",
    "model = SentenceTransformer(model_name, trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5375ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import lzma\n",
    "with lzma.open(\"llm-abstract-sentences.json.xz\", \"rt\") as f:\n",
    "    es = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16f8deb-1857-4add-8e8e-38634c2ef23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270409"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38d3dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.13 s, sys: 56.8 ms, total: 5.18 s\n",
      "Wall time: 4.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Test zur Geschwindigkeitsmessung\n",
    "embeddings = model.encode([e[\"title\"] + \": \" + e[\"text\"] for e in es[0:1000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "536938d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "977adc96-f8f0-400c-bd11-edf5db28e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle Embeddings ausrechnen\n",
    "embeddings = model.encode([e[\"title\"] + \": \" + e[\"text\"] for e in es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d30361ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(\"llm-abstract-sentences-saev2.npy\", \"wb\") as f:\n",
    "    np.save(f, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3afc63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = model.encode(\"How long do I train an LLM?\", prompt_name=\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1da1e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = model.similarity(qe, embeddings).flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52ad1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '2309.03852', 'title': 'FLM-101B: An Open LLM and How to Train It with $100K Budget', 'text': 'FLM-101B: An Open LLM and How to Train It with $100K Budget. Large language models (LLMs) are considered important approaches towards foundational machine intelligence, achieving remarkable success in Natural Language Processing and multimodal tasks, among others. However, the carbon footprints and financial costs originating from heavy pre-training computation is a non-negligible issue. Progressive training methods, inspired by the neurogenesis process that grows neural structures, have shown potential to accelerate LLM pre-training. However, the algorithms, implementation, and practices for progressively training LLMs beyond 100B parameters remain underexplored.'}\n",
      "{'id': '2404.14395', 'title': 'PARAMANU-GANITA: Language Model with Mathematical Capabilities', 'text': 'Paramanu-Ganita took 146 hours of A100 training whereas math specialised LLM, LLEMMA 7B, was trained for 23,000 A100 hours of training equivalent. Thus, our approach of pretraining powerful domain specialised language models from scratch for domain adaptation is much more cost-effective than performing continual training of LLMs for domain adaptation. Hence, we conclude that for strong mathematical reasoning abilities of language model, we do not need giant LLMs and immense computing power to our end. In the end, we want to point out that we have only trained Paramanu-Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model.'}\n",
      "{'id': '2310.00576', 'title': 'GrowLength: Accelerating LLMs Pretraining by Progressively Growing\\n  Training Length', 'text': 'Our method progressively increases the training length throughout the pretraining phase, thereby mitigating computational costs and enhancing efficiency. For instance, it begins with a sequence length of 128 and progressively extends to 4096. This approach enables models to process a larger number of tokens within limited time frames, potentially boosting their performance. In other words, the efficiency gain is derived from training with shorter sequences optimizing the utilization of resources. Our extensive experiments with various state-of-the-art LLMs have revealed that models trained using our method not only converge more swiftly but also exhibit superior performance metrics compared to those trained with existing methods.'}\n",
      "{'id': '2401.06951', 'title': 'E^2-LLM: Efficient and Extreme Length Extension of Large Language Models', 'text': 'E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly.'}\n",
      "{'id': '2407.10827', 'title': 'LLM Circuit Analyses Are Consistent Across Training and Scale', 'text': \"LLM Circuit Analyses Are Consistent Across Training and Scale. Most currently deployed large language models (LLMs) undergo continuous training or additional finetuning. By contrast, most research into LLMs' internal mechanisms focuses on models at one snapshot in time (the end of pre-training), raising the question of whether their results generalize to real-world settings. Existing studies of mechanisms over time focus on encoder-only or toy models, which differ significantly from most deployed models. In this study, we track how model mechanisms, operationalized as circuits, emerge and evolve across 300 billion tokens of training in decoder-only LLMs, in models ranging from 70 million to 2.8 billion parameters.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in similarities.argsort()[::-1][0:5]:\n",
    "    print(es[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cf236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
